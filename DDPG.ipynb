{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DDPG.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OanxcVAWUyz1"
      },
      "source": [
        "# Imports and torch initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4lUj7L6Czob"
      },
      "source": [
        "# https://medium.com/analytics-vidhya/rendering-openai-gym-environments-in-google-colab-9df4e7d6f99f\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install -U colabgymrender\n",
        "!pip install pyglet==1.5.11\n",
        "\n",
        "!pip install box2d-py\n",
        "!pip install gym[Box_2D]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bfs5lFvqRH1x"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from numpy.random import random, randint\n",
        "from colabgymrender.recorder import Recorder\n",
        "from fastprogress.fastprogress import master_bar, progress_bar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jomvj36B2P5D"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.optim import Adam\n",
        "from torch.distributions import Normal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02vAxwM8EiZy",
        "outputId": "fb6791fb-1c85-43cd-964b-308c5ad50987"
      },
      "source": [
        "_ = torch.manual_seed(42)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XqLoGeaU7TF"
      },
      "source": [
        "# Creating policy and env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGXaveWd2iaL"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim, max_action):\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "        self.get_action = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        Returns action\n",
        "        \"\"\"\n",
        "        return self.get_action(state) * self.max_action\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.get_action_value = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        \"\"\"\n",
        "        Returns state-action value\n",
        "        \"\"\"\n",
        "        return self.get_action_value(torch.cat([state, action], axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir4W-ZtrRV_G"
      },
      "source": [
        "class DDPG:\n",
        "    def __init__(self,\n",
        "                 state_dim, action_dim, hidden_dim, max_action,\n",
        "                 buffer_size, batch_size,\n",
        "                 gamma, soft_update_c):\n",
        "        \"\"\"\n",
        "        DDPG policy\n",
        "        \"\"\"\n",
        "\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "\n",
        "        self.replay_buffer = []\n",
        "        self.buffer_size = buffer_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Reward decay\n",
        "        self.gamma = gamma\n",
        "        # Coefficient used in soft model updates\n",
        "        self.soft_update_c = soft_update_c\n",
        "\n",
        "        # Actor model\n",
        "        self.actor = Actor(state_dim, action_dim, hidden_dim, max_action).to(device)\n",
        "        # Frozen actor\n",
        "        self.target_actor = Actor(state_dim, action_dim, hidden_dim, max_action).to(device)\n",
        "\n",
        "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optim = Adam(self.actor.parameters(), lr=1e-2)\n",
        "\n",
        "        # Critic model\n",
        "        self.critic = Critic(state_dim, action_dim, hidden_dim).to(device)\n",
        "        # Frozen critic\n",
        "        self.target_critic = Critic(state_dim, action_dim, hidden_dim).to(device)\n",
        "\n",
        "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optim = Adam(self.critic.parameters(), lr=1e-2)\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        \"\"\"\n",
        "        Switches between training and evaluation\n",
        "        \"\"\"\n",
        "\n",
        "        self.actor.train(mode)\n",
        "        self.target_actor.train(mode)\n",
        "        self.critic.train(mode)\n",
        "        self.target_critic.train(mode)\n",
        "\n",
        "    def add_to_buffer(self, step):\n",
        "        \"\"\"\n",
        "        Adds new step to buffer\n",
        "        \"\"\"\n",
        "\n",
        "        self.replay_buffer.append(step)\n",
        "\n",
        "        # Remove first half if buffer overflowed\n",
        "        if len(self.replay_buffer) > self.buffer_size:\n",
        "            self.replay_buffer = self.replay_buffer[len(self.replay_buffer) // 2:]\n",
        "\n",
        "    def sample_step(self):\n",
        "        \"\"\"\n",
        "        Samples a batch of steps from replay buffer\n",
        "        \"\"\"\n",
        "\n",
        "        idx = np.random.choice(len(self.replay_buffer), self.batch_size)\n",
        "\n",
        "        raw_batch = [[] for _ in range(5)]\n",
        "        for i in idx:\n",
        "            for j in range(5):\n",
        "                raw_batch[j].append(self.replay_buffer[i][j])\n",
        "        \n",
        "        for j in range(5):\n",
        "            raw_batch[j] = np.array(raw_batch[j], copy=False)\n",
        "\n",
        "        return raw_batch\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        \"\"\"\n",
        "        Selects and returns an action as numpy array\n",
        "        \"\"\"\n",
        "\n",
        "        state = torch.tensor(state).float().to(device)\n",
        "        action = self.actor(state).cpu().detach().numpy()\n",
        "        return action.reshape(-1)\n",
        "    \n",
        "    def sync_networks(self):\n",
        "        \"\"\"\n",
        "        Syncronizes frozen networks. Uses soft update\n",
        "        \"\"\"\n",
        "\n",
        "        for param, target_param in zip(self.actor.parameters(), self.target_actor.parameters()):\n",
        "            target_param.data.copy_(self.soft_update_c * target_param.data +\\\n",
        "                                    (1 - self.soft_update_c) * param.data)\n",
        "                \n",
        "        for param, target_param in zip(self.critic.parameters(), self.target_critic.parameters()):\n",
        "            target_param.data.copy_(self.soft_update_c * target_param.data +\\\n",
        "                                    (1 - self.soft_update_c) * param.data)\n",
        "\n",
        "    \n",
        "    def update_networks(self):\n",
        "        \"\"\"\n",
        "        Updates networks with steps from current replay buffer\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            raw_batch = self.sample_step()\n",
        "\n",
        "            state = torch.tensor(raw_batch[0]).float().to(device)\n",
        "            action = torch.tensor(raw_batch[1]).float().to(device)\n",
        "            reward = torch.tensor(raw_batch[2]).float().to(device).unsqueeze(1)\n",
        "            next_state = torch.tensor(raw_batch[3]).float().to(device)\n",
        "            done = torch.tensor(raw_batch[4]).float().to(device).unsqueeze(1)\n",
        "\n",
        "            # Getting target reward\n",
        "            with torch.no_grad():\n",
        "                new_reward = (1 - done) * self.target_critic(next_state,\n",
        "                                                             self.target_actor(next_state))\n",
        "                target_reward = reward + self.gamma * new_reward.detach()\n",
        "\n",
        "            # Updating critic\n",
        "            predicted_reward = self.critic(state, action)\n",
        "            critic_loss = F.mse_loss(target_reward, predicted_reward)\n",
        "\n",
        "            self.critic_optim.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.critic_optim.step()\n",
        "\n",
        "            # Updating actor\n",
        "            actor_loss = -self.critic(state, self.actor(state)).mean()\n",
        "\n",
        "            self.actor_optim.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            self.actor_optim.step()\n",
        "\n",
        "            # Syncronizes frozen models    \n",
        "            self.sync_networks()\n",
        "        \n",
        "            return critic_loss.cpu().item(), actor_loss.cpu().item()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            return None, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thyMHKmSBfiF"
      },
      "source": [
        "recorder_dir = './video'\n",
        "env_rec = Recorder(gym.make('Pendulum-v0'), recorder_dir)\n",
        "env = gym.make('Pendulum-v0').env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snlLjAcCDwCe"
      },
      "source": [
        "action_dim = env.action_space.shape[0]\n",
        "state_dim = env.observation_space.shape[0]\n",
        "max_action = env.action_space.high[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5PLG3igU_tA"
      },
      "source": [
        "# Training code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-OoFRS0jBcO"
      },
      "source": [
        "def get_window_avg(arr, window_size):\n",
        "    return sum(arr[-window_size:]) / min(window_size, len(arr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGMTTr_7E3XG"
      },
      "source": [
        "def train(env, policy, train_noise_std, use_epsilon_greedy=True,\n",
        "          warmup=1000, max_episodes=10000, max_steps=1000, episodes_per_log=100):\n",
        "\n",
        "    # Running average window size\n",
        "    window_size = 100\n",
        "\n",
        "    policy.train()\n",
        "\n",
        "    rewards = []\n",
        "    critic_losses = []\n",
        "    actor_losses = []\n",
        "\n",
        "    for episode in progress_bar(range(1, max_episodes + 1)):\n",
        "        # Initializing new episode\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            # Choosing an action\n",
        "            if episode <= warmup or (use_epsilon_greedy and np.random.rand() < 0.05):\n",
        "                # Epsilon greedy and warmup, increases randomness if actor loss is very small\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = policy.select_action(state)\n",
        "                action += np.random.normal(0, train_noise_std, action_dim)\n",
        "                action = np.clip(action, -1, 1)\n",
        "\n",
        "            # Taking an action\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            # Adding step to buffer\n",
        "            policy.add_to_buffer((state, action, reward, next_state, float(done)))\n",
        "            # Updating state\n",
        "            state = next_state\n",
        "\n",
        "            # Updating rewards\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done or step == max_steps - 1:\n",
        "                break\n",
        "\n",
        "        if episode > warmup:\n",
        "            # Updating networks\n",
        "            critic_loss, actor_loss = policy.update_networks()\n",
        "\n",
        "            # Updating logs\n",
        "            critic_losses.append(critic_loss)\n",
        "            actor_losses.append(actor_loss)\n",
        "            rewards.append(episode_reward)\n",
        "\n",
        "            reward_window_avg = get_window_avg(rewards, window_size)\n",
        "            critic_loss_window_avg = get_window_avg(critic_losses, window_size)\n",
        "            actor_loss_window_avg = get_window_avg(actor_losses, window_size)\n",
        "\n",
        "            if episode % episodes_per_log == 0:\n",
        "                print(f\"Average reward of last {min(window_size, episode)} episodes: {reward_window_avg}\")\n",
        "                print(f\"Average critic loss of last {min(window_size, episode)} episodes: {critic_loss_window_avg}\")\n",
        "                print(f\"Average actor loss of last {min(window_size, episode)} episodes: {actor_loss_window_avg}\")\n",
        "                print(\"---\")\n",
        "            \n",
        "            # Useful if you're doing Box2D tasks\n",
        "            # if reward_window_avg >= 200 and episode >= window_size:\n",
        "            #     print(\"Solved!\")\n",
        "            #     return rewards, critic_losses, actor_losses\n",
        "    \n",
        "    print(\"Max episodes reached\")\n",
        "    return rewards, critic_losses, actor_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-bI4g_CcYyH"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U5PmbLTgYSi"
      },
      "source": [
        "policy = DDPG(\n",
        "    state_dim, action_dim, 128, max_action,\n",
        "    1e6, 256,\n",
        "    0.99, 0.995,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_vtXxGYcXOT"
      },
      "source": [
        "rewards, critic_losses, actor_losses = train(env, policy, 0.2, True, 2000, 4000, 500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WJkUx51WvJQ"
      },
      "source": [
        "rewards, critic_losses, actor_losses = train(env, policy, 0.05, False, 0, 2000, 500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pSZXY9KrueT"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPpZEqlB9XXI"
      },
      "source": [
        "policy.train(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMnboj83TBk1"
      },
      "source": [
        "# In case you saved the model\n",
        "policy.actor = torch.load(\"actor.pth\")\n",
        "policy.critic =  torch.load(\"critic.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KSroLvTLMGA"
      },
      "source": [
        "done = False\n",
        "n = 1\n",
        "observation = env_rec.reset()\n",
        "total_reward = 0\n",
        "while not done:\n",
        "  action = policy.select_action(torch.Tensor(observation).to(device))\n",
        "  observation, reward, done, _ = env_rec.step(action)\n",
        "  n += 1\n",
        "\n",
        "  if n <= 100:\n",
        "      total_reward += reward\n",
        "\n",
        "env_rec.play()\n",
        "print(f\"100 episode reward: {total_reward}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}